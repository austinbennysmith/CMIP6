{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAO_prediction_variance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3d1pGxyMChsEKcWgm+4J+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austinbennysmith/CMIP6/blob/main/NAO_prediction_variance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvvEjQLhQN0Y"
      },
      "source": [
        "## This notebook will:\n",
        "* **Plot anomalies of the NAO time series, and running averages of those anomalies (both plotted for each model, averaged across ensemble members)**\n",
        "* **Plot the standard deviation across ensemble members for each model over time (and include + or - one standard deviation on plots of the anomalies)**\n",
        "* **Do bootstrapping of the model output and plot 95% Confidence intervals with the anomalies graphs**\n",
        "\n",
        "In the plots of SSP5-8.5,the y-axis scale is different for different graphs. It appears that there is more of a trend for e.g. CanESM5 than many others, but is this actually true? Or does the scale just make it look that way?\n",
        "\n",
        "Dimension naming issue with the last models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5fcK0NVIcid"
      },
      "source": [
        "## Installs and imports:\n",
        "!pip install fsspec\n",
        "!pip install netCDF4\n",
        "!apt-get -qq install python-cartopy python3-cartopy;\n",
        "!pip uninstall -y shapely;\n",
        "!pip install shapely --no-binary shapely;\n",
        "!pip install eofs\n",
        "! pip install --upgrade xarray zarr gcsfs cftime nc-time-axis\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import zarr\n",
        "import fsspec\n",
        "import gcsfs\n",
        "from eofs.xarray import Eof\n",
        "from nc_time_axis import NetCDFTimeConverter, CalendarDateTime\n",
        "import cftime\n",
        "from scipy import stats\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.rcParams['figure.figsize'] = 12, 6\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6duCq30_IhrB",
        "outputId": "053f77ee-b602-4ed2-e297-f42abd089e89"
      },
      "source": [
        "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv') \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activity_id</th>\n",
              "      <th>institution_id</th>\n",
              "      <th>source_id</th>\n",
              "      <th>experiment_id</th>\n",
              "      <th>member_id</th>\n",
              "      <th>table_id</th>\n",
              "      <th>variable_id</th>\n",
              "      <th>grid_label</th>\n",
              "      <th>zstore</th>\n",
              "      <th>dcpp_init_year</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HighResMIP</td>\n",
              "      <td>CMCC</td>\n",
              "      <td>CMCC-CM2-HR4</td>\n",
              "      <td>highresSST-present</td>\n",
              "      <td>r1i1p1f1</td>\n",
              "      <td>Amon</td>\n",
              "      <td>tauv</td>\n",
              "      <td>gn</td>\n",
              "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20170706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HighResMIP</td>\n",
              "      <td>CMCC</td>\n",
              "      <td>CMCC-CM2-HR4</td>\n",
              "      <td>highresSST-present</td>\n",
              "      <td>r1i1p1f1</td>\n",
              "      <td>Amon</td>\n",
              "      <td>huss</td>\n",
              "      <td>gn</td>\n",
              "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20170706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HighResMIP</td>\n",
              "      <td>CMCC</td>\n",
              "      <td>CMCC-CM2-HR4</td>\n",
              "      <td>highresSST-present</td>\n",
              "      <td>r1i1p1f1</td>\n",
              "      <td>Amon</td>\n",
              "      <td>rlus</td>\n",
              "      <td>gn</td>\n",
              "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20170706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HighResMIP</td>\n",
              "      <td>CMCC</td>\n",
              "      <td>CMCC-CM2-HR4</td>\n",
              "      <td>highresSST-present</td>\n",
              "      <td>r1i1p1f1</td>\n",
              "      <td>Amon</td>\n",
              "      <td>rlds</td>\n",
              "      <td>gn</td>\n",
              "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20170706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HighResMIP</td>\n",
              "      <td>CMCC</td>\n",
              "      <td>CMCC-CM2-HR4</td>\n",
              "      <td>highresSST-present</td>\n",
              "      <td>r1i1p1f1</td>\n",
              "      <td>Amon</td>\n",
              "      <td>psl</td>\n",
              "      <td>gn</td>\n",
              "      <td>gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20170706</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  activity_id institution_id  ... dcpp_init_year   version\n",
              "0  HighResMIP           CMCC  ...            NaN  20170706\n",
              "1  HighResMIP           CMCC  ...            NaN  20170706\n",
              "2  HighResMIP           CMCC  ...            NaN  20170706\n",
              "3  HighResMIP           CMCC  ...            NaN  20170706\n",
              "4  HighResMIP           CMCC  ...            NaN  20170706\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI-8vEpvihkx"
      },
      "source": [
        "## **List of all experiments in the archive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI_YaUn3ikxn",
        "outputId": "3d76b8b4-57ec-4bc6-9d36-9edb3ccd4c07"
      },
      "source": [
        "uexp = []\n",
        "for i in df.experiment_id:\n",
        "  if i not in uexp:\n",
        "    uexp.append(i)\n",
        "for i in uexp:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "histSST\n",
            "piClim-CH4\n",
            "piClim-NTCF\n",
            "piClim-control\n",
            "ssp370\n",
            "hist-1950HC\n",
            "piClim-2xDMS\n",
            "piClim-2xdust\n",
            "piClim-2xfire\n",
            "piClim-2xss\n",
            "piClim-BC\n",
            "piClim-HC\n",
            "piClim-N2O\n",
            "piClim-OC\n",
            "piClim-SO2\n",
            "piClim-aer\n",
            "hist-piNTCF\n",
            "hist-piAer\n",
            "histSST-1950HC\n",
            "histSST-piAer\n",
            "histSST-piCH4\n",
            "histSST-piNTCF\n",
            "histSST-piO3\n",
            "piClim-2xNOx\n",
            "piClim-2xVOC\n",
            "piClim-NOx\n",
            "piClim-O3\n",
            "piClim-VOC\n",
            "ssp370-lowNTCF\n",
            "ssp370SST-lowCH4\n",
            "ssp370SST-lowNTCF\n",
            "ssp370SST-ssp126Lu\n",
            "ssp370SST\n",
            "ssp370pdSST\n",
            "1pctCO2-bgc\n",
            "1pctCO2-rad\n",
            "esm-ssp585\n",
            "hist-bgc\n",
            "1pctCO2-cdr\n",
            "esm-pi-CO2pulse\n",
            "esm-pi-cdr-pulse\n",
            "amip-4xCO2\n",
            "amip-future4K\n",
            "amip-m4K\n",
            "amip-p4K\n",
            "amip\n",
            "abrupt-2xCO2\n",
            "abrupt-solp4p\n",
            "abrupt-0p5xCO2\n",
            "amip-lwoff\n",
            "amip-p4K-lwoff\n",
            "aqua-4xCO2\n",
            "abrupt-solm4p\n",
            "aqua-control-lwoff\n",
            "aqua-control\n",
            "aqua-p4K-lwoff\n",
            "aqua-p4K\n",
            "1pctCO2\n",
            "abrupt-4xCO2\n",
            "historical\n",
            "piControl\n",
            "esm-hist\n",
            "esm-piControl\n",
            "amip-hist\n",
            "historical-cmip5\n",
            "piControl-cmip5\n",
            "esm-piControl-spinup\n",
            "piControl-spinup\n",
            "hist-GHG\n",
            "hist-nat\n",
            "historical-ext\n",
            "hist-aer\n",
            "hist-CO2\n",
            "hist-GHG-cmip5\n",
            "hist-aer-cmip5\n",
            "hist-nat-cmip5\n",
            "hist-sol\n",
            "hist-stratO3\n",
            "hist-totalO3\n",
            "hist-volc\n",
            "ssp245-GHG\n",
            "ssp245-aer\n",
            "ssp245-nat\n",
            "ssp245-stratO3\n",
            "ssp245-cov-fossil\n",
            "ssp245-cov-modgreen\n",
            "ssp245-cov-strgreen\n",
            "ssp245-covid\n",
            "dcppA-hindcast\n",
            "dcppA-assim\n",
            "dcppC-hindcast-noAgung\n",
            "dcppC-hindcast-noElChichon\n",
            "dcppC-hindcast-noPinatubo\n",
            "dcppC-amv-neg\n",
            "dcppC-amv-pos\n",
            "dcppC-amv-ExTrop-neg\n",
            "dcppC-amv-ExTrop-pos\n",
            "dcppC-amv-Trop-neg\n",
            "dcppC-amv-Trop-pos\n",
            "dcppC-atl-control\n",
            "dcppC-atl-pacemaker\n",
            "dcppC-ipv-NexTrop-neg\n",
            "dcppC-ipv-NexTrop-pos\n",
            "dcppC-ipv-neg\n",
            "dcppC-ipv-pos\n",
            "dcppC-pac-control\n",
            "dcppC-pac-pacemaker\n",
            "faf-heat\n",
            "faf-passiveheat\n",
            "faf-stress\n",
            "faf-water\n",
            "faf-all\n",
            "faf-heat-NA0pct\n",
            "faf-heat-NA50pct\n",
            "hist-resIPO\n",
            "highresSST-present\n",
            "highresSST-future\n",
            "hist-1950\n",
            "control-1950\n",
            "land-hist\n",
            "deforest-globe\n",
            "esm-ssp585-ssp126Lu\n",
            "hist-noLu\n",
            "land-noLu\n",
            "land-hist-altStartYear\n",
            "ssp126-ssp370Lu\n",
            "ssp370-ssp126Lu\n",
            "omip1\n",
            "futSST-pdSIC\n",
            "pdSST-futAntSIC\n",
            "pdSST-futArcSIC\n",
            "pdSST-pdSIC\n",
            "pdSST-piAntSIC\n",
            "pdSST-piArcSIC\n",
            "piSST-pdSIC\n",
            "piSST-piSIC\n",
            "pa-futArcSIC\n",
            "pa-pdSIC\n",
            "lgm\n",
            "lig127k\n",
            "midHolocene\n",
            "past1000\n",
            "piClim-4xCO2\n",
            "piClim-anthro\n",
            "piClim-ghg\n",
            "piClim-histall\n",
            "piClim-lu\n",
            "piClim-histaer\n",
            "piClim-histghg\n",
            "piClim-histnat\n",
            "ssp126\n",
            "ssp245\n",
            "ssp585\n",
            "ssp119\n",
            "ssp434\n",
            "ssp460\n",
            "ssp534-over\n",
            "rcp26-cmip5\n",
            "rcp45-cmip5\n",
            "rcp85-cmip5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji9-LU9yIj6y",
        "outputId": "47cb102b-4da9-417f-c1db-e87dd3570864"
      },
      "source": [
        "from datetime import date, timedelta\n",
        "import netCDF4 as nc\n",
        "# psl_ssp585 = df.query(\"variable_id == 'psl' & experiment_id == 'ssp585' & institution_id == 'NCAR' & source_id == 'CESM2'\")\n",
        "# psl_ssp585 = df.query(\"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon' & source_id == 'CESM2'\")\n",
        "psl_ssp585 = df.query(\"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon' & source_id == 'CESM2'\")\n",
        "for i in range(len(psl_ssp585)):\n",
        "  print(i)\n",
        "print(len(psl_ssp585.zstore.values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APP88Dt12J7u"
      },
      "source": [
        "Plotting a histogram of the ensemble size for all the models available for the SSP5-8.5 experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQEBW7oyvVVf"
      },
      "source": [
        "ssp = df.query(\"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon'\")\n",
        "unique_sources = []\n",
        "histlist = []\n",
        "for i in ssp['source_id']:\n",
        "  if i not in unique_sources:\n",
        "    unique_sources.append(i)\n",
        "print(unique_sources)\n",
        "\n",
        "for i in unique_sources:\n",
        "  x = ssp.query(\"source_id ==\"+\"'\"+i+\"'\")\n",
        "  print(str(len(x))+': '+i)\n",
        "  histlist.append(len(x))\n",
        "plt.figure(figsize=(10,  10))\n",
        "bins = np.arange(60)\n",
        "print(bins)\n",
        "plt.hist(histlist, bins)\n",
        "plt.title('Ensemble size histogram')\n",
        "plt.xlabel('# of Members')\n",
        "plt.ylabel('Count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaxeEf5S_3r8"
      },
      "source": [
        "## **Plotting the time series of anomalies averaged across ensemble members for a model, along with a running mean of the anomales:**\n",
        "\n",
        "(Here I take the running mean of the anomalies and then normalize it, which apparently produces different results than if I normalize the anomalies and then take the running mean...why is this? The next code cell after this one has two extra lines (which are commented out) that do this process in the other order so you can see the difference)\n",
        "\n",
        "NOTE: I've added a line that selects just the time range from January 1st 2015 to January 1st 2101. For most of the datasets this is unnecessary, but I realized that run 20 of CanESM5 has data through 2300, and I would like to only focus on the 21st century in order to make comparisons to other data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4lMLykXbE_J"
      },
      "source": [
        "### This cell prints a list of the types of datetime data in the archive.\n",
        "# This is helpful because in the following cell I have a series of if/elif statements that restrict the time range to 2015-2100, since a few of the datasets go longer than this and I am trying to average across time series.\n",
        "# When running this type of code for different experiments, I need to make sure there are enough if statements in the following cell to account for every type of datetime data in the archive.\n",
        "\n",
        "def get_types(experiment):\n",
        "  global uniquetypes\n",
        "  test = df.query(\"variable_id == 'psl' & experiment_id == \"+\"'\"+experiment+\"'\"+\" & table_id == 'Amon'\")\n",
        "  usource = []\n",
        "  for m in test.source_id:\n",
        "    if m not in usource:\n",
        "      usource.append(m)\n",
        "\n",
        "  uniquetypes = []\n",
        "  for source in usource:\n",
        "    print(source)\n",
        "    abc = \"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon' & source_id == \"+\"'\"+source+\"'\"\n",
        "    psl_ssp585 = df.query(abc)\n",
        "    # print('length: ', len(psl_ssp585.zstore.values))\n",
        "    for i in range(len(psl_ssp585.zstore.values)):\n",
        "      zstore = psl_ssp585.zstore.values[i]\n",
        "      mapper = fsspec.get_mapper(zstore)\n",
        "      ds = xr.open_zarr(mapper, consolidated=True, decode_times=True) # Make decode_times=True to convert dates to datetime objects?? The problem is, Colab doesn't seem to like my installation of nc-time-axis. I could try a Jupyter Binder, or keep Googling to try to solve this.\n",
        "      psl = ds.psl.sel(lat=slice(20,80))\n",
        "      psl = psl.where((ds.lon >= 270) | (ds.lon <= 40), drop=True)\n",
        "      if type(psl.time.values[0]) not in uniquetypes:\n",
        "        uniquetypes.append(type(psl.time.values[0]))\n",
        "      if len(psl.time)!=1032 and len(psl.time)!=1020:\n",
        "        print('time length:', len(psl.time), 'INDEX:', i)\n",
        "    # smoothplot()\n",
        "  print('UNIQUE TYPES: ', uniquetypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCGvOLFAD-hL"
      },
      "source": [
        "Plotting the anomalies of the PC time series (and their running average) and the same for the standard deviation across ensemble members of the anomalies\n",
        "\n",
        "* Standard deviation is consistently higher in the Winter than the summer because there are more storms in the Winter, which causes more short-term variability in the NAOI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMPzJCMkJKFy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "71025fb3-9b7f-4ff1-bd57-15ecadda0a75"
      },
      "source": [
        "def smoothplot():\n",
        "  global psl\n",
        "  global eigenpatterns\n",
        "  global meanpatterns\n",
        "  eigenpatterns = {}\n",
        "  SEASONS = ['DJF', 'MAM', 'JJA', 'SON']\n",
        "  # Moving average code from https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-numpy-scipy\n",
        "  def moving_average(x, w):\n",
        "      return np.convolve(x, np.ones(w), 'valid') / w\n",
        "  eigenpatterns['singVal 0']={\n",
        "      'anomalies': {},\n",
        "  }\n",
        "  for key in eigenpatterns['singVal 0']:\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0'][key][season] = []\n",
        "  for member in range(len(psl_ssp585.zstore.values)): \n",
        "    print(member)\n",
        "    # Accessing the file, getting it to just the lats and lons I want:\n",
        "    zstore = psl_ssp585.zstore.values[member]\n",
        "    mapper = fsspec.get_mapper(zstore)\n",
        "    ds = xr.open_zarr(mapper, consolidated=True, decode_times=True) # Make decode_times=True to convert dates to datetime objects?? The problem is, Colab doesn't seem to like my installation of nc-time-axis. I could try a Jupyter Binder, or keep Googling to try to solve this.\n",
        "    psl = ds.psl.sel(lat=slice(20,80))\n",
        "    psl = psl.where((ds.lon >= 270) | (ds.lon <= 40), drop=True)\n",
        "    lat = ds.lat.sel(lat=slice(20,80))\n",
        "    lon = ds.lon.where((ds.lon >= 270) | (ds.lon <= 40), drop=True) # Not sure exactly what drop means, but I think it doesn't matter\n",
        "    if isinstance((psl.time.values[member]), np.datetime64):\n",
        "      psl = psl.sel(time=slice('2015-01-01T12:00:00.000000000', '2101-01-01T12:00:00.000000000'))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.DatetimeNoLeap):\n",
        "      psl = psl.sel(time=slice(cftime.DatetimeNoLeap(2015, 1, 1, 12, 0, 0, 0), cftime.DatetimeNoLeap(2101, 1, 1, 12, 0, 0, 0)))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.DatetimeJulian):\n",
        "      psl = psl.sel(time=slice(cftime.DatetimeJulian(2015, 1, 1, 12, 0, 0, 0), cftime.DatetimeJulian(2101, 1, 1, 12, 0, 0, 0)))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.Datetime360Day):\n",
        "      psl = psl.sel(time=slice(cftime.Datetime360Day(2015, 1, 1, 12, 0, 0, 0), cftime.Datetime360Day(2101, 1, 1, 12, 0, 0, 0)))\n",
        "\n",
        "\n",
        "    # Useful stackexchange post: https://stackoverflow.com/questions/40272222/select-xarray-pandas-index-based-on-specific-months\n",
        "    def is_djf(month):\n",
        "      return (month>=12) | ((month>=1) & (month<=2))\n",
        "    def is_mam(month):\n",
        "      return (month>=3) & (month<=5)\n",
        "    def is_jja(month):\n",
        "      return (month>=6) & (month<=8)\n",
        "    def is_son(month):\n",
        "      return (month>=9) & (month<=11)\n",
        "    psl_djf = psl.sel(time=is_djf(psl['time.month']))\n",
        "    psl_mam = psl.sel(time=is_mam(psl['time.month']))\n",
        "    psl_jja = psl.sel(time=is_jja(psl['time.month']))\n",
        "    psl_son = psl.sel(time=is_son(psl['time.month']))\n",
        "\n",
        "\n",
        "    psl_allseasons = {\n",
        "        'DJF': psl_djf,\n",
        "        'MAM': psl_mam,\n",
        "        'JJA': psl_jja,\n",
        "        'SON': psl_son\n",
        "    }\n",
        "    century_trends = {\n",
        "        'DJF': {},\n",
        "        'JJA': {},\n",
        "        'MAM': {},\n",
        "        'SON': {}\n",
        "    }\n",
        "    for key in psl_allseasons.keys():\n",
        "      tlength = len(psl_allseasons[key].time) # The psl_WEIGHTED dictionary does not contain DataArray objects, so I can't just get the time from that.\n",
        "      latlength = len(psl_allseasons[key].lat)\n",
        "      lonlength = len(psl_allseasons[key].lon)\n",
        "      psl2d = np.reshape(psl_allseasons[key].values, (tlength, latlength*lonlength)) # This is the only line in the for loop I changed when adding in the weighting\n",
        "      psl2d = np.matrix.transpose(psl2d)\n",
        "      Unow, snow, VTnow = LA.svd(psl2d, full_matrices=False)\n",
        "      century_trends[key]['U'] = Unow\n",
        "      century_trends[key]['s'] = snow\n",
        "      century_trends[key]['VT'] = VTnow\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0']['anomalies'][season].append(century_trends[season]['VT'][0]-np.mean(century_trends[season]['VT'][0]))\n",
        "    \n",
        "  meanpatterns = {}\n",
        "  for singVal in eigenpatterns:\n",
        "    meanpatterns[singVal] = {}\n",
        "    meanpatterns[singVal]['std'] = {}\n",
        "    for kind in eigenpatterns[singVal]:\n",
        "      meanpatterns[singVal][kind] = {}\n",
        "      for season in eigenpatterns[singVal][kind]:\n",
        "        stacked = np.stack([i for i in eigenpatterns[singVal][kind][season]], axis=0)\n",
        "        meanpatterns[singVal][kind][season] = stacked.mean(axis=0)\n",
        "        meanpatterns[singVal]['std'][season] = stacked.std(axis=0)\n",
        "  meanpatterns['singVal 0']['smoothed'] = {}\n",
        "  for season in SEASONS:\n",
        "    nao_smoothed = moving_average(meanpatterns['singVal 0']['anomalies'][season], 30)\n",
        "    meanpatterns['singVal 0']['smoothed'][season] = nao_smoothed\n",
        "    \n",
        "  # for member in range(len(psl_ssp585.zstore.values)):\n",
        "      # nao_smoothed = moving_average(eigenpatterns['singVal 0']['anomalies'][season][member], 30) # Doing the moving average for 30 points because each season is 3 months per year - so this is a 10 year running mean\n",
        "      # eigenpatterns['singVal 0']['smoothed'][season].append(nao_smoothed/np.std(nao_smoothed))\n",
        "    \n",
        "  fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "  axlist = axs.flatten()\n",
        "  for season in SEASONS:\n",
        "    # x = np.arange(0, 495)\n",
        "    x = np.arange(0, len(nao_smoothed))\n",
        "    axlist[SEASONS.index(season)].fill_between(x, 0, meanpatterns['singVal 0']['smoothed'][season], where=meanpatterns['singVal 0']['smoothed'][season]>0, color='r', label='10 year running mean')\n",
        "    axlist[SEASONS.index(season)].fill_between(x, 0, meanpatterns['singVal 0']['smoothed'][season], where=meanpatterns['singVal 0']['smoothed'][season]<0, color='b', label='10 year running mean')\n",
        "    axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['anomalies'][season], 'silver', zorder=0, label='Unsmoothed')\n",
        "    # plt.plot(x)\n",
        "    axlist[SEASONS.index(season)].set_title('NAO Index (PC 1) '+season)\n",
        "    axlist[SEASONS.index(season)].set_ylabel('Right Singular Vector Anomalies')\n",
        "    axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "    axlist[SEASONS.index(season)].legend()\n",
        "  plt.show()\n",
        "  fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "  axlist = axs.flatten()\n",
        "  for season in SEASONS:\n",
        "    # x = np.arange(0, 495)\n",
        "    x = np.arange(0, len(nao_smoothed))\n",
        "    axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['std'][season], 'silver', zorder=0, label='Unsmoothed')\n",
        "    axlist[SEASONS.index(season)].plot(moving_average(meanpatterns['singVal 0']['std'][season], 30), 'red', label='Smoothed')\n",
        "    # plt.plot(x)\n",
        "    axlist[SEASONS.index(season)].set_title('Prediction standard deviation '+season)\n",
        "    axlist[SEASONS.index(season)].set_ylabel('Standard deviation across ensemble members')\n",
        "    axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "    axlist[SEASONS.index(season)].legend()\n",
        "  plt.show()\n",
        "smoothplot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-37bb541587c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0maxlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSEASONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0msmoothplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-37bb541587c1>\u001b[0m in \u001b[0;36msmoothplot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mzstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsl_ssp585\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Make decode_times=True to convert dates to datetime objects?? The problem is, Colab doesn't seem to like my installation of nc-time-axis. I could try a Jupyter Binder, or keep Googling to try to solve this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mpsl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpsl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlon\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m270\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlon\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mopen_zarr\u001b[0;34m(store, group, synchronizer, chunks, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated, overwrite_encoded_chunks, chunk_store, decode_timedelta, use_cftime, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mbackend_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0muse_cftime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_decode_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;31m# Ensure source filename always stored in dataset object (GH issue #2550)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mmaybe_decode_store\u001b[0;34m(store, chunks)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdrop_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0muse_cftime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         )\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf\u001b[0;34m(obj, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mdrop_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0muse_cftime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     )\n\u001b[1;32m    600\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf_variables\u001b[0;34m(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mstack_char_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack_char_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0muse_cftime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_timedelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         )\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecode_coords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf_variable\u001b[0;34m(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFTimedeltaCoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFDatetimeCoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_for_decoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, variable, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"units\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mcalendar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"calendar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_decode_cf_datetime_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalendar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m             transform = partial(\n\u001b[1;32m    465\u001b[0m                 \u001b[0mdecode_cf_datetime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/coding/times.py\u001b[0m in \u001b[0;36m_decode_cf_datetime_dtype\u001b[0;34m(data, units, calendar, use_cftime)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImplicitToExplicitIndexingAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     example_value = np.concatenate(\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mfirst_n_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/core/formatting.py\u001b[0m in \u001b[0;36mfirst_n_items\u001b[0;34m(array, n_desired)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_indexer_at_least_n_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_desired\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_desired\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicIndexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVectorizedIndexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             return array.vindex[\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zarr/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, selection)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_fields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_basic_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_basic_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEllipsis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zarr/core.py\u001b[0m in \u001b[0;36mget_basic_selection\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             return self._get_basic_selection_nd(selection=selection, out=out,\n\u001b[0;32m--> 697\u001b[0;31m                                                 fields=fields)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_basic_selection_zd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zarr/core.py\u001b[0m in \u001b[0;36m_get_basic_selection_nd\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_orthogonal_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zarr/core.py\u001b[0m in \u001b[0;36m_get_selection\u001b[0;34m(self, indexer, out, fields)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mlchunk_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlchunk_selection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlout_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             self._chunk_getitems(lchunk_coords, lchunk_selection, out, lout_selection,\n\u001b[0;32m-> 1035\u001b[0;31m                                  drop_axes=indexer.drop_axes, fields=fields)\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zarr/core.py\u001b[0m in \u001b[0;36m_chunk_getitems\u001b[0;34m(self, lchunk_coords, lchunk_selection, out, lout_selection, drop_axes, fields)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mckeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlchunk_coords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m         \u001b[0mcdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"omit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mckey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_select\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_select\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlchunk_selection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlout_selection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mckey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcdatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/mapping.py\u001b[0m in \u001b[0;36mgetitems\u001b[0;34m(self, keys, on_error)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0moe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mon_error\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mon_error\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"return\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_exceptions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(self, path, recursive, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mon_error\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mmaybe_sync\u001b[0;34m(func, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# run the awaitable on the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# just call the blocking function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHWrMnuN3sAX"
      },
      "source": [
        "Making the aforementioned plots for every model that ran the SSP5-8.5 experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGQu3g8Kp3yl"
      },
      "source": [
        "test = df.query(\"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon'\")\n",
        "usource = []\n",
        "for m in test.source_id:\n",
        "  if m not in usource:\n",
        "    usource.append(m)\n",
        "\n",
        "for source in usource:\n",
        "  print(source)\n",
        "  abc = \"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon' & source_id == \"+\"'\"+source+\"'\"\n",
        "  psl_ssp585 = df.query(abc)\n",
        "  smoothplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awE3KIZvmKW6"
      },
      "source": [
        "## **Same plots as before, but with the averages plus or minus the standard deviation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG8r2vsEmJVd"
      },
      "source": [
        "def smoothplot():\n",
        "  global psl\n",
        "  global eigenpatterns\n",
        "  global meanpatterns\n",
        "  eigenpatterns = {}\n",
        "  SEASONS = ['DJF', 'MAM', 'JJA', 'SON']\n",
        "  # Moving average code from https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-numpy-scipy\n",
        "  def moving_average(x, w):\n",
        "      return np.convolve(x, np.ones(w), 'valid') / w\n",
        "  eigenpatterns['singVal 0']={\n",
        "      'anomalies': {},\n",
        "  }\n",
        "  for key in eigenpatterns['singVal 0']:\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0'][key][season] = []\n",
        "  for member in range(len(psl_ssp585.zstore.values)): \n",
        "    print(member)\n",
        "    # Accessing the file, getting it to just the lats and lons I want:\n",
        "    zstore = psl_ssp585.zstore.values[member]\n",
        "    mapper = fsspec.get_mapper(zstore)\n",
        "    ds = xr.open_zarr(mapper, consolidated=True, decode_times=True) # Make decode_times=True to convert dates to datetime objects?? The problem is, Colab doesn't seem to like my installation of nc-time-axis. I could try a Jupyter Binder, or keep Googling to try to solve this.\n",
        "    psl = ds.psl.sel(lat=slice(20,80))\n",
        "    psl = psl.where((ds.lon >= 270) | (ds.lon <= 40), drop=True)\n",
        "    lat = ds.lat.sel(lat=slice(20,80))\n",
        "    lon = ds.lon.where((ds.lon >= 270) | (ds.lon <= 40), drop=True) # Not sure exactly what drop means, but I think it doesn't matter\n",
        "    if isinstance((psl.time.values[member]), np.datetime64):\n",
        "      psl = psl.sel(time=slice('2015-01-01T12:00:00.000000000', '2101-01-01T12:00:00.000000000'))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.DatetimeNoLeap):\n",
        "      psl = psl.sel(time=slice(cftime.DatetimeNoLeap(2015, 1, 1, 12, 0, 0, 0), cftime.DatetimeNoLeap(2101, 1, 1, 12, 0, 0, 0)))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.DatetimeJulian):\n",
        "      psl = psl.sel(time=slice(cftime.DatetimeJulian(2015, 1, 1, 12, 0, 0, 0), cftime.DatetimeJulian(2101, 1, 1, 12, 0, 0, 0)))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.Datetime360Day):\n",
        "      psl = psl.sel(time=slice(cftime.Datetime360Day(2015, 1, 1, 12, 0, 0, 0), cftime.Datetime360Day(2101, 1, 1, 12, 0, 0, 0)))\n",
        "\n",
        "\n",
        "    # Useful stackexchange post: https://stackoverflow.com/questions/40272222/select-xarray-pandas-index-based-on-specific-months\n",
        "    def is_djf(month):\n",
        "      return (month>=12) | ((month>=1) & (month<=2))\n",
        "    def is_mam(month):\n",
        "      return (month>=3) & (month<=5)\n",
        "    def is_jja(month):\n",
        "      return (month>=6) & (month<=8)\n",
        "    def is_son(month):\n",
        "      return (month>=9) & (month<=11)\n",
        "    psl_djf = psl.sel(time=is_djf(psl['time.month']))\n",
        "    psl_mam = psl.sel(time=is_mam(psl['time.month']))\n",
        "    psl_jja = psl.sel(time=is_jja(psl['time.month']))\n",
        "    psl_son = psl.sel(time=is_son(psl['time.month']))\n",
        "\n",
        "\n",
        "    psl_allseasons = {\n",
        "        'DJF': psl_djf,\n",
        "        'MAM': psl_mam,\n",
        "        'JJA': psl_jja,\n",
        "        'SON': psl_son\n",
        "    }\n",
        "    century_trends = {\n",
        "        'DJF': {},\n",
        "        'JJA': {},\n",
        "        'MAM': {},\n",
        "        'SON': {}\n",
        "    }\n",
        "    for key in psl_allseasons.keys():\n",
        "      tlength = len(psl_allseasons[key].time) # The psl_WEIGHTED dictionary does not contain DataArray objects, so I can't just get the time from that.\n",
        "      latlength = len(psl_allseasons[key].lat)\n",
        "      lonlength = len(psl_allseasons[key].lon)\n",
        "      psl2d = np.reshape(psl_allseasons[key].values, (tlength, latlength*lonlength)) # This is the only line in the for loop I changed when adding in the weighting\n",
        "      psl2d = np.matrix.transpose(psl2d)\n",
        "      Unow, snow, VTnow = LA.svd(psl2d, full_matrices=False)\n",
        "      century_trends[key]['U'] = Unow\n",
        "      century_trends[key]['s'] = snow\n",
        "      century_trends[key]['VT'] = VTnow\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0']['anomalies'][season].append(century_trends[season]['VT'][0]-np.mean(century_trends[season]['VT'][0]))\n",
        "    \n",
        "  meanpatterns = {}\n",
        "  for singVal in eigenpatterns:\n",
        "    meanpatterns[singVal] = {}\n",
        "    meanpatterns[singVal]['std'] = {}\n",
        "    for kind in eigenpatterns[singVal]:\n",
        "      meanpatterns[singVal][kind] = {}\n",
        "      for season in eigenpatterns[singVal][kind]:\n",
        "        stacked = np.stack([i for i in eigenpatterns[singVal][kind][season]], axis=0)\n",
        "        meanpatterns[singVal][kind][season] = stacked.mean(axis=0)\n",
        "        meanpatterns[singVal]['std'][season] = stacked.std(axis=0)\n",
        "  meanpatterns['singVal 0']['smoothed'] = {}\n",
        "  for season in SEASONS:\n",
        "    nao_smoothed = moving_average(meanpatterns['singVal 0']['anomalies'][season], 30)\n",
        "    meanpatterns['singVal 0']['smoothed'][season] = nao_smoothed\n",
        "    \n",
        "  # for member in range(len(psl_ssp585.zstore.values)):\n",
        "      # nao_smoothed = moving_average(eigenpatterns['singVal 0']['anomalies'][season][member], 30) # Doing the moving average for 30 points because each season is 3 months per year - so this is a 10 year running mean\n",
        "      # eigenpatterns['singVal 0']['smoothed'][season].append(nao_smoothed/np.std(nao_smoothed))\n",
        "    \n",
        "  fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "  axlist = axs.flatten()\n",
        "  for season in SEASONS:\n",
        "    # x = np.arange(0, 495)\n",
        "    x = np.arange(0, len(nao_smoothed))\n",
        "    std_below = meanpatterns['singVal 0']['smoothed'][season]-moving_average(meanpatterns['singVal 0']['std'][season], 30)\n",
        "    std_above = meanpatterns['singVal 0']['smoothed'][season]+moving_average(meanpatterns['singVal 0']['std'][season], 30)\n",
        "    axlist[SEASONS.index(season)].fill_between(x, 0, meanpatterns['singVal 0']['smoothed'][season], where=meanpatterns['singVal 0']['smoothed'][season]>0, color='r', label='10 year running mean')\n",
        "    axlist[SEASONS.index(season)].fill_between(x, 0, meanpatterns['singVal 0']['smoothed'][season], where=meanpatterns['singVal 0']['smoothed'][season]<0, color='b', label='10 year running mean')\n",
        "    # axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['smoothed'][season], color='k', label = '10 year running mean')\n",
        "    axlist[SEASONS.index(season)].plot(std_below, color='g')\n",
        "    axlist[SEASONS.index(season)].plot(std_above, color = 'g')\n",
        "    axlist[SEASONS.index(season)].fill_between(x, std_below, std_above, color='g', alpha = 0.3)\n",
        "    axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['anomalies'][season], 'silver', zorder=0, label='Unsmoothed')\n",
        "    # plt.plot(x)\n",
        "    axlist[SEASONS.index(season)].set_title('NAO Index (PC 1) '+season)\n",
        "    axlist[SEASONS.index(season)].set_ylabel('Right Singular Vector Anomalies')\n",
        "    axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "    axlist[SEASONS.index(season)].legend()\n",
        "  plt.show()\n",
        "  fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "  axlist = axs.flatten()\n",
        "  for season in SEASONS:\n",
        "    # x = np.arange(0, 495)\n",
        "    x = np.arange(0, len(nao_smoothed))\n",
        "    axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['std'][season], 'silver', zorder=0, label='Unsmoothed')\n",
        "    axlist[SEASONS.index(season)].plot(moving_average(meanpatterns['singVal 0']['std'][season], 30), 'red', label='Smoothed')\n",
        "    # plt.plot(x)\n",
        "    axlist[SEASONS.index(season)].set_title('Prediction standard deviation '+season)\n",
        "    axlist[SEASONS.index(season)].set_ylabel('Standard deviation across ensemble members')\n",
        "    axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "    axlist[SEASONS.index(season)].legend()\n",
        "  plt.show()\n",
        "smoothplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32VmxaDRewB"
      },
      "source": [
        "**Adding in Confidence intervals:**\n",
        "\n",
        "(as can be seen in the graphs later on, this is not completely working yet (the confidence intervals of the standard deviation do not always include the standard deviation graph))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QaLZw664MiE"
      },
      "source": [
        "def smoothplot():\n",
        "  global psl\n",
        "  global eigenpatterns\n",
        "  global meanpatterns\n",
        "  eigenpatterns = {}\n",
        "  SEASONS = ['DJF', 'MAM', 'JJA', 'SON']\n",
        "  # Moving average code from https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-numpy-scipy\n",
        "  def moving_average(x, w):\n",
        "      return np.convolve(x, np.ones(w), 'valid') / w\n",
        "  eigenpatterns['singVal 0']={\n",
        "      'anomalies': {},\n",
        "  }\n",
        "  for key in eigenpatterns['singVal 0']:\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0'][key][season] = []\n",
        "  for member in range(len(psl_ssp585.zstore.values)): \n",
        "    print(member)\n",
        "    # Accessing the file, getting it to just the lats and lons I want:\n",
        "    zstore = psl_ssp585.zstore.values[member]\n",
        "    mapper = fsspec.get_mapper(zstore)\n",
        "    ds = xr.open_zarr(mapper, consolidated=True, decode_times=True) # Make decode_times=True to convert dates to datetime objects?? The problem is, Colab doesn't seem to like my installation of nc-time-axis. I could try a Jupyter Binder, or keep Googling to try to solve this.\n",
        "    psl = ds.psl.sel(lat=slice(20,80))\n",
        "    psl = psl.where((ds.lon >= 270) | (ds.lon <= 40), drop=True)\n",
        "    lat = ds.lat.sel(lat=slice(20,80))\n",
        "    lon = ds.lon.where((ds.lon >= 270) | (ds.lon <= 40), drop=True) # Not sure exactly what drop means, but I think it doesn't matter\n",
        "    if isinstance((psl.time.values[member]), np.datetime64):\n",
        "      psl = psl.sel(time=slice('2015-01-01T12:00:00.000000000', '2101-01-01T12:00:00.000000000'))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.DatetimeNoLeap):\n",
        "      psl = psl.sel(time=slice(cftime.DatetimeNoLeap(2015, 1, 1, 12, 0, 0, 0), cftime.DatetimeNoLeap(2101, 1, 1, 12, 0, 0, 0)))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.DatetimeJulian):\n",
        "      psl = psl.sel(time=slice(cftime.DatetimeJulian(2015, 1, 1, 12, 0, 0, 0), cftime.DatetimeJulian(2101, 1, 1, 12, 0, 0, 0)))\n",
        "    elif isinstance((psl.time.values[member]), cftime._cftime.Datetime360Day):\n",
        "      psl = psl.sel(time=slice(cftime.Datetime360Day(2015, 1, 1, 12, 0, 0, 0), cftime.Datetime360Day(2101, 1, 1, 12, 0, 0, 0)))\n",
        "\n",
        "\n",
        "    # Useful stackexchange post: https://stackoverflow.com/questions/40272222/select-xarray-pandas-index-based-on-specific-months\n",
        "    def is_djf(month):\n",
        "      return (month>=12) | ((month>=1) & (month<=2))\n",
        "    def is_mam(month):\n",
        "      return (month>=3) & (month<=5)\n",
        "    def is_jja(month):\n",
        "      return (month>=6) & (month<=8)\n",
        "    def is_son(month):\n",
        "      return (month>=9) & (month<=11)\n",
        "    psl_djf = psl.sel(time=is_djf(psl['time.month']))\n",
        "    psl_mam = psl.sel(time=is_mam(psl['time.month']))\n",
        "    psl_jja = psl.sel(time=is_jja(psl['time.month']))\n",
        "    psl_son = psl.sel(time=is_son(psl['time.month']))\n",
        "\n",
        "\n",
        "    psl_allseasons = {\n",
        "        'DJF': psl_djf,\n",
        "        'MAM': psl_mam,\n",
        "        'JJA': psl_jja,\n",
        "        'SON': psl_son\n",
        "    }\n",
        "    century_trends = {\n",
        "        'DJF': {},\n",
        "        'JJA': {},\n",
        "        'MAM': {},\n",
        "        'SON': {}\n",
        "    }\n",
        "    for key in psl_allseasons.keys():\n",
        "      tlength = len(psl_allseasons[key].time) # The psl_WEIGHTED dictionary does not contain DataArray objects, so I can't just get the time from that.\n",
        "      latlength = len(psl_allseasons[key].lat)\n",
        "      lonlength = len(psl_allseasons[key].lon)\n",
        "      psl2d = np.reshape(psl_allseasons[key].values, (tlength, latlength*lonlength)) # This is the only line in the for loop I changed when adding in the weighting\n",
        "      psl2d = np.matrix.transpose(psl2d)\n",
        "      Unow, snow, VTnow = LA.svd(psl2d, full_matrices=False)\n",
        "      century_trends[key]['U'] = Unow\n",
        "      century_trends[key]['s'] = snow\n",
        "      century_trends[key]['VT'] = VTnow\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0']['anomalies'][season].append(century_trends[season]['VT'][0]-np.mean(century_trends[season]['VT'][0]))\n",
        "    \n",
        "  meanpatterns = {}\n",
        "  for singVal in eigenpatterns:\n",
        "    meanpatterns[singVal] = {}\n",
        "    meanpatterns[singVal]['std'] = {}\n",
        "    for kind in eigenpatterns[singVal]:\n",
        "      meanpatterns[singVal][kind] = {}\n",
        "      meanpatterns[singVal]['stacked'] = {}\n",
        "      ## The following seems to be unnecessary:\n",
        "      meanpatterns[singVal]['bootstrapping std'] = {}\n",
        "      for season in eigenpatterns[singVal][kind]:\n",
        "        stacked = np.stack([i for i in eigenpatterns[singVal][kind][season]], axis=0)\n",
        "        meanpatterns[singVal][kind][season] = stacked.mean(axis=0)\n",
        "        meanpatterns[singVal]['std'][season] = stacked.std(axis=0)\n",
        "        # stacked = np.stack([i for i in eigenpatterns[singVal][kind][season]], axis=0)\n",
        "        meanpatterns[singVal]['stacked'][season] = stacked\n",
        "        sample_hist = []\n",
        "\n",
        "  meanpatterns['singVal 0']['smoothed'] = {}\n",
        "  for season in SEASONS:\n",
        "    nao_smoothed = moving_average(meanpatterns['singVal 0']['anomalies'][season], 30)\n",
        "    meanpatterns['singVal 0']['smoothed'][season] = nao_smoothed\n",
        "  \n",
        "  Sboot = {\n",
        "      'DJF': [],\n",
        "      'JJA': [],\n",
        "      'SON': [],\n",
        "      'MAM': []\n",
        "  }\n",
        "  fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "  axlist = axs.flatten()\n",
        "  for season in SEASONS:\n",
        "    ## Loading in the data (a stacked ndarray of NAOI time series):\n",
        "    data = meanpatterns[singVal]['stacked'][season]\n",
        "    ## Setting up indices to be resampled. len(data) is the number of ensemble members, and so if there are 50 members the indices I'm resampling should be 0 through 50. Thus if I resample 1000 times I get 1000 different stacked ndarrays, each with a randomly chosen set of 50 members (chosen with replacement)\n",
        "    indices = np.arange(len(data))\n",
        "\n",
        "    ## I will include the means of the resampled data as well as the standard deviations\n",
        "    resample_dict = {\n",
        "        'resample means': {},\n",
        "        'resample std': {},\n",
        "    }\n",
        "    for i in range(1000):\n",
        "      index_resample = resample(indices, replace=True, n_samples=len(data))\n",
        "      # print(index_resample)\n",
        "      resample_stack = np.stack([data[index_resample[j]] for j in range(len(index_resample))], axis=0)\n",
        "      resample_means = np.mean(resample_stack, axis=0)\n",
        "      resample_dict['resample means'].append(resample_means)\n",
        "      resample_std = np.std(resample_stack, axis=0)\n",
        "      resample_dict['resample std'].append(resample_std)\n",
        "\n",
        "    resample_dict['resample means'] = np.stack(resample_dict['resample means'], axis=0)\n",
        "    bootmean = resample_dict['resample means'].mean(axis=0)\n",
        "    boot_std = resample_dict['resample means'].std(axis=0)\n",
        "    bootmean_smoothed = moving_average(bootmean, 30)\n",
        "\n",
        "    resample_dict['resample std'] = np.stack(resample_dict['resample std'], axis=0)\n",
        "    Sbootmean = resample_dict['resample std'].mean(axis=0)\n",
        "    Sboot_std = resample_dict['resample std'].std(axis=0)\n",
        "    Sbootmean_smoothed = moving_average(Sbootmean, 30)\n",
        "\n",
        "    # What's the best point at which to do the moving average?\n",
        "    # Here I'm adding/subtracting the mean to/from the standard deviation, then taking the moving average. Is that correct? Or should I do that in reverse order? Does it matter?\n",
        "    boot_below = bootmean-2*boot_std\n",
        "    boot_above = bootmean+2*boot_std\n",
        "    boot_below_smoothed = moving_average(boot_below, 30)\n",
        "    boot_above_smoothed = moving_average(boot_above, 30)\n",
        "    x = np.arange(len(boot_below_smoothed))\n",
        "\n",
        "    Sboot_below = Sbootmean-2*Sboot_std\n",
        "    Sboot_above = Sbootmean+2*Sboot_std\n",
        "    Sboot_below_smoothed = moving_average(Sboot_below, 30)\n",
        "    Sboot_above_smoothed = moving_average(Sboot_above, 30)\n",
        "    Sboot[season]['below'] = Sboot_below_smoothed\n",
        "    Sboot[season]['above'] = Sboot_above_smoothed\n",
        "\n",
        "    # axlist[SEASONS.index(season)].plot(moving_average(data.mean(axis=0), 30), 'k', lw=5, zorder=3)\n",
        "    axlist[SEASONS.index(season)].plot(moving_average(meanpatterns['singVal 0']['anomalies'][season], 30), 'k', lw=5, zorder=3)\n",
        "    axlist[SEASONS.index(season)].plot(moving_average(boot_below, 30), 'g', lw=3, label='95% CI')\n",
        "    axlist[SEASONS.index(season)].plot(moving_average(boot_above, 30), 'g', lw=3, label='95% CI')\n",
        "    axlist[SEASONS.index(season)].fill_between(x, 0, moving_average(data.mean(axis=0), 30), where=moving_average(data.mean(axis=0), 30)>0, color='r', label='10 year running mean')\n",
        "    axlist[SEASONS.index(season)].fill_between(x, 0, moving_average(data.mean(axis=0), 30), moving_average(data.mean(axis=0), 30)<0, color='b', label='10 year running mean')\n",
        "    axlist[SEASONS.index(season)].fill_between(x, boot_below_smoothed, boot_above_smoothed, color='g', alpha=0.3)\n",
        "    axlist[SEASONS.index(season)].legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  # fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "  # axlist = axs.flatten()\n",
        "  # for season in SEASONS:\n",
        "  #   # x = np.arange(0, 495)\n",
        "  #   x = np.arange(0, len(nao_smoothed))\n",
        "  #   axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['std'][season], 'silver', zorder=0, label='Unsmoothed')\n",
        "  #   axlist[SEASONS.index(season)].plot(moving_average(meanpatterns['singVal 0']['std'][season], 30), 'k', lw=5, label='Smoothed')\n",
        "  #   axlist[SEASONS.index(season)].plot(moving_average(Sboot_below, 30), 'g', lw=3, label='95% CI')\n",
        "  #   axlist[SEASONS.index(season)].plot(moving_average(Sboot_above, 30), 'g', lw=3, label='95% CI')\n",
        "  #   axlist[SEASONS.index(season)].fill_between(x, Sboot_below_smoothed, Sboot_above_smoothed, color='g', alpha=0.3)\n",
        "  #   # plt.plot(x)\n",
        "  #   axlist[SEASONS.index(season)].set_title('Prediction standard deviation '+season)\n",
        "  #   axlist[SEASONS.index(season)].set_ylabel('Standard deviation across ensemble members')\n",
        "  #   axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "  #   axlist[SEASONS.index(season)].legend()\n",
        "  # plt.show()\n",
        "  fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "  axlist = axs.flatten()\n",
        "  for season in SEASONS:\n",
        "    # x = np.arange(0, 495)\n",
        "    x = np.arange(0, len(nao_smoothed))\n",
        "    axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['std'][season], 'silver', zorder=0, label='Unsmoothed')\n",
        "    axlist[SEASONS.index(season)].plot(moving_average(meanpatterns['singVal 0']['std'][season], 30), 'k', lw=5, label='Smoothed')\n",
        "    axlist[SEASONS.index(season)].plot(Sboot[season]['below'], 'g', lw=3, label='95% CI')\n",
        "    axlist[SEASONS.index(season)].plot(Sboot[season]['above'], 'g', lw=3, label='95% CI')\n",
        "    axlist[SEASONS.index(season)].fill_between(x, Sboot[season]['below'], Sboot[season]['above'], color='g', alpha=0.3)\n",
        "    # plt.plot(x)\n",
        "    axlist[SEASONS.index(season)].set_title('Prediction standard deviation '+season)\n",
        "    axlist[SEASONS.index(season)].set_ylabel('Standard deviation across ensemble members')\n",
        "    axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "    axlist[SEASONS.index(season)].legend()\n",
        "  plt.show()\n",
        "smoothplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRXbS7kmwFu3"
      },
      "source": [
        "The following cell does the plots of the anomalies & standard deviation & confidence intervals for a single dataset rather than all of them (this is just useful for debugging...and there does seem to still be a bug)\n",
        "\n",
        "NEXT:\n",
        "\n",
        "* Make sure everything plotted (2 cells down) the way I expected (without the one that used to be the last one) check\n",
        "\n",
        "* Try out the cell below, in which I implemented confidence intervals for the resampled standard deviation. Debug.\n",
        "\n",
        "* TEST FOR SIZE OF THE ARRAY TO MAKE SURE IT HAS 1000 ROWS\n",
        "\n",
        "* Add in unsmoothed data in light grey in the background of the NAOI mean plot??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KodAUXdk9-8d"
      },
      "source": [
        "## Randomly choose a set of ensemble members with replacement, then get the mean of that randomly chosen sample and append the mean to a list\n",
        "\n",
        "def moving_average(x, w):\n",
        "  return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "SEASONS = ['DJF', 'MAM', 'JJA', 'SON']\n",
        "fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "axlist = axs.flatten()\n",
        "for season in SEASONS:\n",
        "  data = meanpatterns[singVal]['stacked'][season]\n",
        "  # data[0].shape\n",
        "  indices = np.arange(len(data))\n",
        "\n",
        "  resample_dict = {\n",
        "      'resample means': [],\n",
        "      'resample std': [],\n",
        "  }\n",
        "  for i in range(1000):\n",
        "    index_resample = resample(indices, replace=True, n_samples=len(data))\n",
        "    # print(index_resample)\n",
        "    resample_stack = np.stack([data[index_resample[j]] for j in range(len(index_resample))], axis=0)\n",
        "    resample_means = np.mean(resample_stack, axis=0)\n",
        "    resample_stds = np.std(resample_stack, axis=0)\n",
        "    resample_dict['resample means'].append(resample_means)\n",
        "    resample_dict['resample std'].append(resample_stds)\n",
        "    # resample_std = np.std(resample_stack, axis=0)\n",
        "\n",
        "  resample_dict['resample means'] = np.stack(resample_dict['resample means'], axis=0)\n",
        "  bootmean = resample_dict['resample means'].mean(axis=0)\n",
        "  bootmean_std = resample_dict['resample means'].std(axis=0)\n",
        "  bootmean_smoothed = moving_average(bootmean, 30)\n",
        "\n",
        "  resample_dict['resample std'] = np.stack(resample_dict['resample std'], axis=0)\n",
        "  bootstd = resample_dict['resample std'].mean(axis=0)\n",
        "  bootstd_std = resample_dict['resample std'].std(axis=0)\n",
        "  bootstd_smoothed = moving_average(bootstd, 30)\n",
        "  \n",
        "\n",
        "  # What's the best point at which to do the moving average?\n",
        "  # Here I'm adding/subtracting the mean to/from the standard deviation, then taking the moving average. Is that correct? Or should I do that in reverse order? Does it matter?\n",
        "  bootmean_below = bootmean-2*bootmean_std\n",
        "  bootmean_above = bootmean+2*bootmean_std\n",
        "  bootmean_below_smoothed = moving_average(bootmean_below, 30)\n",
        "  bootmean_above_smoothed = moving_average(bootmean_above, 30)\n",
        "  x = np.arange(len(bootmean_below_smoothed))\n",
        "\n",
        "  bootstd_below = bootstd-2*bootstd_std\n",
        "  bootstd_above = bootstd+2*bootstd_std\n",
        "  bootstd_below_smoothed = moving_average(bootstd_below, 30)\n",
        "  bootstd_above_smoothed = moving_average(bootstd_above, 30)\n",
        "\n",
        "  axlist[SEASONS.index(season)].plot(moving_average(data.mean(axis=0), 30), 'k', lw=5, zorder=3)\n",
        "  axlist[SEASONS.index(season)].plot(moving_average(bootmean_below, 30), 'g', lw=3, label='95% CI')\n",
        "  axlist[SEASONS.index(season)].plot(moving_average(bootmean_above, 30), 'g', lw=3, label='95% CI')\n",
        "  axlist[SEASONS.index(season)].fill_between(x, 0, moving_average(data.mean(axis=0), 30), where=moving_average(data.mean(axis=0), 30)>0, color='r', label='10 year running mean')\n",
        "  axlist[SEASONS.index(season)].fill_between(x, 0, moving_average(data.mean(axis=0), 30), moving_average(data.mean(axis=0), 30)<0, color='b', label='10 year running mean')\n",
        "  axlist[SEASONS.index(season)].fill_between(x, bootmean_below_smoothed, bootmean_above_smoothed, color='g', alpha=0.3)\n",
        "  axlist[SEASONS.index(season)].legend()\n",
        "plt.show()\n",
        "fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "axlist = axs.flatten()\n",
        "for season in SEASONS:\n",
        "  axlist[SEASONS.index(season)].plot(moving_average(data.std(axis=0), 30), 'red', label='Smoothed')\n",
        "  axlist[SEASONS.index(season)].plot(data.std(axis=0), 'silver', zorder=0, label='Unsmoothed')\n",
        "  axlist[SEASONS.index(season)].fill_between(x, bootstd_below_smoothed, bootstd_above_smoothed, color='g', alpha=0.3)\n",
        "  # axlist[SEASONS.index(season)].plot(meanpatterns['singVal 0']['std'][season], 'silver', zorder=0, label='Unsmoothed')\n",
        "  # axlist[SEASONS.index(season)].plot(moving_average(meanpatterns['singVal 0']['std'][season], 30), 'red', label='Smoothed')\n",
        "  # axlist[SEASONS.index(season)].set_title('Prediction standard deviation '+season)\n",
        "  # axlist[SEASONS.index(season)].set_ylabel('Standard deviation across ensemble members')\n",
        "  # axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "  axlist[SEASONS.index(season)].legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEDNZC566SrH"
      },
      "source": [
        "Making the confidence intervals plots for every model that ran the SSP5-8.5 experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN-WjrzjtZO-"
      },
      "source": [
        "test = df.query(\"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon'\")\n",
        "usource = []\n",
        "for m in test.source_id:\n",
        "  if m not in usource:\n",
        "    usource.append(m)\n",
        "\n",
        "for source in usource:\n",
        "  print(source)\n",
        "  if source == 'MCM-UA-1-0': # This model has different naming for its dimensions, making it annoying. I might fix this later (although this one only has one ensemble member so it may not be useful anyway)\n",
        "    continue\n",
        "  abc = \"variable_id == 'psl' & experiment_id == 'ssp585' & table_id == 'Amon' & source_id == \"+\"'\"+source+\"'\"\n",
        "  psl_ssp585 = df.query(abc)\n",
        "  smoothplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6AzT7tV6Y8-"
      },
      "source": [
        "For the SSP5-3.4 experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFXEZeBxl6IB"
      },
      "source": [
        "test = df.query(\"variable_id == 'psl' & experiment_id == 'ssp534' & table_id == 'Amon'\")\n",
        "usource = []\n",
        "for m in test.source_id:\n",
        "  if m not in usource:\n",
        "    usource.append(m)\n",
        "\n",
        "for source in usource:\n",
        "  print(source)\n",
        "  if source == 'MCM-UA-1-0': # This model has different naming for its dimensions, making it annoying. I might fix this later (although this one only has one ensemble member so it may not be useful anyway)\n",
        "    continue\n",
        "  abc = \"variable_id == 'psl' & experiment_id == 'ssp534' & table_id == 'Amon' & source_id == \"+\"'\"+source+\"'\"\n",
        "  psl_ssp585 = df.query(abc)\n",
        "  smoothplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mdQrRW-u7IF"
      },
      "source": [
        "## **The following cell does the plotting for each individual member instead of averaging, with similar architecture to the previous cells**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsTlmjjz_tgi"
      },
      "source": [
        "def smoothplot():\n",
        "  global psl\n",
        "  global eigenpatterns\n",
        "  eigenpatterns = {}\n",
        "  SEASONS = ['DJF', 'MAM', 'JJA', 'SON']\n",
        "  # Moving average code from https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-numpy-scipy\n",
        "  def moving_average(x, w):\n",
        "      return np.convolve(x, np.ones(w), 'valid') / w\n",
        "  eigenpatterns['singVal 0']={\n",
        "      'anomalies': {},\n",
        "      'normalized': {},\n",
        "      'smoothed': {}\n",
        "  }\n",
        "  for key in eigenpatterns['singVal 0']:\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0'][key][season] = []\n",
        "  for member in range(len(psl_ssp585.zstore.values)): \n",
        "    print(member)\n",
        "    # Accessing the file, getting it to just the lats and lons I want:\n",
        "    zstore = psl_ssp585.zstore.values[member]\n",
        "    mapper = fsspec.get_mapper(zstore)\n",
        "    ds = xr.open_zarr(mapper, consolidated=True, decode_times=True) # Make decode_times=True to convert dates to datetime objects?? The problem is, Colab doesn't seem to like my installation of nc-time-axis. I could try a Jupyter Binder, or keep Googling to try to solve this.\n",
        "    psl = ds.psl.sel(lat=slice(20,80))\n",
        "    psl = psl.where((ds.lon >= 270) | (ds.lon <= 40), drop=True)\n",
        "    lat = ds.lat.sel(lat=slice(20,80))\n",
        "    lon = ds.lon.where((ds.lon >= 270) | (ds.lon <= 40), drop=True) # Not sure exactly what drop means, but I think it doesn't matter\n",
        "\n",
        "\n",
        "    # Useful stackexchange post: https://stackoverflow.com/questions/40272222/select-xarray-pandas-index-based-on-specific-months\n",
        "    def is_djf(month):\n",
        "      return (month>=12) | ((month>=1) & (month<=2))\n",
        "    def is_mam(month):\n",
        "      return (month>=3) & (month<=5)\n",
        "    def is_jja(month):\n",
        "      return (month>=6) & (month<=8)\n",
        "    def is_son(month):\n",
        "      return (month>=9) & (month<=11)\n",
        "    psl_djf = psl.sel(time=is_djf(psl['time.month']))\n",
        "    psl_mam = psl.sel(time=is_mam(psl['time.month']))\n",
        "    psl_jja = psl.sel(time=is_jja(psl['time.month']))\n",
        "    psl_son = psl.sel(time=is_son(psl['time.month']))\n",
        "\n",
        "\n",
        "    psl_allseasons = {\n",
        "        'DJF': psl_djf,\n",
        "        'MAM': psl_mam,\n",
        "        'JJA': psl_jja,\n",
        "        'SON': psl_son\n",
        "    }\n",
        "    century_trends = {\n",
        "        'DJF': {},\n",
        "        'JJA': {},\n",
        "        'MAM': {},\n",
        "        'SON': {}\n",
        "    }\n",
        "    for key in psl_allseasons.keys():\n",
        "      tlength = len(psl_allseasons[key].time) # The psl_WEIGHTED dictionary does not contain DataArray objects, so I can't just get the time from that.\n",
        "      latlength = len(psl_allseasons[key].lat)\n",
        "      lonlength = len(psl_allseasons[key].lon)\n",
        "      psl2d = np.reshape(psl_allseasons[key].values, (tlength, latlength*lonlength)) # This is the only line in the for loop I changed when adding in the weighting\n",
        "      psl2d = np.matrix.transpose(psl2d)\n",
        "      Unow, snow, VTnow = LA.svd(psl2d, full_matrices=False)\n",
        "      century_trends[key]['U'] = Unow\n",
        "      century_trends[key]['s'] = snow\n",
        "      century_trends[key]['VT'] = VTnow\n",
        "    for season in SEASONS:\n",
        "      eigenpatterns['singVal 0']['anomalies'][season].append(century_trends[season]['VT'][0]-np.mean(century_trends[season]['VT'][0]))\n",
        "      eigenpatterns['singVal 0']['normalized'][season].append(eigenpatterns['singVal 0']['anomalies'][season][member]/np.std(century_trends[season]['VT'][0]))  \n",
        "    \n",
        "      \n",
        "    \n",
        "  # for member in range(len(psl_ssp585.zstore.values)):\n",
        "      nao_smoothed = moving_average(eigenpatterns['singVal 0']['anomalies'][season][member], 30) # Doing the moving average for 30 points because each season is 3 months per year - so this is a 10 year running mean\n",
        "      eigenpatterns['singVal 0']['smoothed'][season].append(nao_smoothed/np.std(nao_smoothed))\n",
        "      # Above, I took the running mean of the anomalies, then normalized. The following two lines will do this in reverse order. Why are the results different? Qualitatively they are the same, though, so I'm not worried in the short term.\n",
        "      # nao_smoothed = moving_average(eigenpatterns['singVal 0']['normalized'][season][member], 30) # Doing the moving average for 30 points because each season is 3 months per year - so this is a 10 year running mean\n",
        "      # eigenpatterns['singVal 0']['smoothed'][season].append(moving_average(eigenpatterns['singVal 0']['normalized'][season][member], 30))\n",
        "    fig, axs = plt.subplots(2, 2, figsize=[24, 12])\n",
        "    axlist = axs.flatten()\n",
        "    for season in SEASONS:\n",
        "      # x = np.arange(0, 495)\n",
        "      x = np.arange(0, len(nao_smoothed))\n",
        "      axlist[SEASONS.index(season)].fill_between(x, 0, eigenpatterns['singVal 0']['smoothed'][season][member], where=eigenpatterns['singVal 0']['smoothed'][season][member]>0, color='r', label='10 year running mean')\n",
        "      axlist[SEASONS.index(season)].fill_between(x, 0, eigenpatterns['singVal 0']['smoothed'][season][member], where=eigenpatterns['singVal 0']['smoothed'][season][member]<0, color='b', label='10 year running mean')\n",
        "      axlist[SEASONS.index(season)].plot(eigenpatterns['singVal 0']['normalized'][season][member], 'silver', zorder=0, label='Unsmoothed')\n",
        "      # plt.plot(x)\n",
        "      axlist[SEASONS.index(season)].set_title('NAO Index (PC 1) '+season)\n",
        "      axlist[SEASONS.index(season)].set_ylabel('Right Singular Vector Normalized Anomalies')\n",
        "      axlist[SEASONS.index(season)].set_xlabel('Time')\n",
        "      axlist[SEASONS.index(season)].legend()\n",
        "    plt.show()\n",
        "smoothplot()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}